{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pL7OrdnLVoy"
   },
   "source": [
    "# Laboratoire 6: Réseaux de neurones récurrents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sftgX3Cu0v-R"
   },
   "source": [
    "## Références\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "https://distill.pub/2016/augmented-rnns/\n",
    "\n",
    "https://github.com/mila-udem/ecolehiver2018/blob/master/Tutoriaux/Hiver18_0308_tutorial_rnn_lstm_solution.ipynb\n",
    "\n",
    "### jean.philippe.reid@rd.mila.quebec\n",
    "\n",
    "### Mars 2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from deeplib.mila import display_logo\n",
    "display_logo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_tMgB2wTQTF"
   },
   "source": [
    "## Module utilitaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBgc1wsCtAGU"
   },
   "source": [
    "Premièrement, installons pytorch et certains modules nécessaires pour compléter ce tutoriel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0iF-E5xOQRe_"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "from torch import optim\n",
    "torch.backends.cudnn.version()\n",
    "\n",
    "cuda = True\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WXqWpBdaQm0-"
   },
   "outputs": [],
   "source": [
    "def normalized(x, y):\n",
    "    '''Cette fonction normalize un le tenseur 'x'. '''\n",
    "    x = (x - torch.min(x,0)[0].view(1,x.shape[1])) / ((torch.max(x,0)[0] - torch.min(x,0)[0]).view(1,x.shape[1]))\n",
    "\n",
    "    return x, x.sum(dim=1)\n",
    "\n",
    "def standardized(x, y):\n",
    "    '''  Cette fonction standardize un le tenseur 'x'. '''  \n",
    "    x = (x - torch.mean(x, 0).view(1,x.shape[1])) / torch.std(x, 0).view(1,x.shape[1])\n",
    "\n",
    "    return x, x.sum(dim=1)\n",
    "\n",
    "def data_set(N, T, interval):\n",
    "    ''' \n",
    "    inputs : \n",
    "    N : nombre de données \n",
    "    T : longueur de la séquence\n",
    "    interval : l'intervalle dans lequel les nombres seront tirés pour générer les séquences. \n",
    "    \n",
    "    outputs: retourne les séquences (xx) et les cibles (yy), en format torch.tensor \n",
    "    '''\n",
    "    \n",
    "    x = (torch.Tensor(N, T).random_(0, 2 * interval[1]) + interval[0])\n",
    "   \n",
    "    return x, x.sum(dim=1)\n",
    "  \n",
    "def print_sequence(x, y):\n",
    "    '''\n",
    "    x : Une séquence particulière, i.e dim(x) = 1 x T\n",
    "    y : Cible liée à cette même séquence, i.e. dim(y) = 1\n",
    "    retourne une série de caractères illustrant la séquence \n",
    "    dans un format convivial. \n",
    "    '''\n",
    "    n = x.shape[0]\n",
    "    for i, x_ in enumerate(x):\n",
    "        if i == 0 : \n",
    "            string=' ' + str(x_)\n",
    "        else: \n",
    "            string=string + ' + '+str(x_)\n",
    "\n",
    "    return string+' = ' + str(y)\n",
    "  \n",
    "def adjust_lr(optimizer, lr0, epoch, total_epochs):\n",
    "    '''\n",
    "    Cette fonction diminue le taux d'apprentissage suivant une fonction \n",
    "    exponentielle avec le nombre d'époque. \n",
    "    \n",
    "    optimizer: e.g. optim.SGD(... )\n",
    "    lr0 : taux d'apprentissage initial\n",
    "    epoch : époque à laquelle la mise à jour est effectuée. \n",
    "    total epochs: nombre d'époque totale\n",
    "    \n",
    "    exemple : \n",
    "    \n",
    "    new_learning_rate = adjust_lr(optimizer, 0.01, e_, 100)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    lr = lr0 * (0.36 ** (epoch / float(total_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def batch_loss(data_batch,model): \n",
    "    '''\n",
    "    Cette fonction calcule le loss d'un jeu de donnée divisé en plusieurs   \n",
    "    batchs. C'est nécessaire pour traiter de grandes quantités de données.     \n",
    "    '''\n",
    "    loss = 0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(data_batch):\n",
    "            x, y = model.input_format(x, y)\n",
    "            out = model(x)\n",
    "            loss += model.criterion(out, y)\n",
    "            n += 1\n",
    "        return loss.item() / n\n",
    "\n",
    "\n",
    "def adjust_fontsize(ax):\n",
    "    '''\n",
    "    Par déformation professionnelle, j'ai le souci de préparer de belles figures. \n",
    "    Cette fonction est un clin d'oeil pour mon ancien superviseur de thèse.\n",
    "    '''\n",
    "    for ax in ax:\n",
    "        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] \n",
    "                 + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            item.set_fontsize(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51tYmwJ2qoqU"
   },
   "source": [
    "## Objectif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDMnGX4iqoqV"
   },
   "source": [
    "L'objectif de ce tutoriel est de construire un modèle capable d'additionner ou soustraire une série de nombres. Ce jeu de données est facile à générer et nous permet de tester la capacité de plusieurs algorithmes tels que,\n",
    "\n",
    "* RNN, LSTM \n",
    "* MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jS8xrTCXqoqW"
   },
   "source": [
    "Le jeu de données est constitué d'une séquence de nombre de longueurs *seq_len* , à laquelle une cible est associée. Dans l'exemple ci-dessous, la i$^{eme}$ composante des données est explicitement détaillée (voir la Fig. 1). \n",
    "\n",
    "\\begin{align}  \n",
    "\\mathrm x^{(i)} &= \\left[ 4,-1,15,24\\right], \\mathrm x^{(i)} \\in \\mathbb R^{d_0} \\\\ \n",
    "\\mathrm y^{(i)} &= 42, \\mathrm y^{(i)} \\in \\mathbb R \n",
    "\\end{align}\n",
    "\n",
    "Il est important de noter que chaque composante du vecteur $\\mathrm x^{(i)}$, peut être de plusieurs dimensions, c'est-à-dire $x^{(i)}_j \\in \\mathbb R^{d_1}$ où $d_1 > 1$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QzmAxECSqoqX"
   },
   "source": [
    "Ce jeu de données peut ainsi servir à entrainer un modèle de réseau de neurones récurrents (RNN), tel que le _long short term memory_(LSTM). Dans ce cas, chaque nombre sera l'entrée d'une couche cachée (_hidden layer_) de dimension $h_d$. \n",
    "\n",
    "Comme la cible est un nombre réel, il est nécessaire de rajouter une couche linéaire au modèle pour \"ajuster\" les dimensions (voir la Fig.2). Cette notion sera expliquée à l'aide d'un exemple détaillé ci-bas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDCPei0QqoqW"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.001.jpeg)\n",
    "\n",
    "Fig.1 : Jeu de données considéré. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ok4sQtHUqoqY"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.002.jpeg)\n",
    "\n",
    "\n",
    "Fig.2 : Schéma d'un réseau récurrent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCnneiTsqoqY"
   },
   "source": [
    "## Réseau de neurones récurrent (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAisYYriqoqZ"
   },
   "source": [
    "### Générer une couche LSTM\n",
    "### lstm = nn.LSTM(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u93VR8oVqoqa"
   },
   "source": [
    "Une couche LSTM peut incorporer plusieurs paramètres dont certains régis par le jeu de données (*input_size*), mais aussi d'autres paramètres essentiels pour optimiser la capacité du modèle, tels que *hidden_size*, *num_layers*, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FgCALpkgqoqb"
   },
   "source": [
    "### Entrées d'une couche LSTM (*inputs*)\n",
    "____ = <font color='red'>lstm</font>(*input*, (h0,c0))__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXaWaGleqoqb"
   },
   "source": [
    "En plus des données (_input_), il est aussi possible d'initialiser les tenseurs h_0, c_0 définis comme les *hidden* et *cell states*,  paramètres essentiels aux LSTMs. \n",
    "\n",
    "__Dans le cas où $h_0$ et $c_0$ ne sont pas définis, le module LSTM utilisera les valeurs par défaut, i.e. 0.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrRFyn_Tqoqc"
   },
   "source": [
    "### Donnée d'entrée (*input*)\n",
    "__<font color='red'>input</font> =  torch.Tensor(seq_len, batch_size, input_size)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKuc_SyMqoqd"
   },
   "source": [
    "Il est nécessaire de réorganiser les données d'entrées (input) selon trois paramètres : \n",
    "\n",
    "* la longueur de la séquence (seq_len)\n",
    "* la grandeur du lot (batch, i.e. batch_size)\n",
    "* les dimensions des entrées (input_size)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dh9ca_HMiKuC"
   },
   "source": [
    "### Donnée de sortie (*output*)\n",
    "### output = lstm(*input*, (h0,c0))\n",
    "__<font color='red'>output</font> = torch.tensor(seq_len,batch_size, hidden_size x num_directions)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGShtHdArgvw"
   },
   "source": [
    "Les dimensions du tenseur \"output_0\" sont déterminées par: (*seq_len*, *batch*, *hidden_size* $\\times$ *num_directions*). \n",
    "\n",
    "Dans le cas qui nous intéresse, num_directions $ = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvtzfm1qaj92"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.003.jpeg)\n",
    "\n",
    "Fig. 3: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9P1NDwZqqoqc"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.004.jpeg)\n",
    "\n",
    "Fig. 4: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-bi9ydcqoqq"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.005.jpeg)\n",
    "\n",
    "Fig.5 : http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Maj8vgFlyQlo"
   },
   "source": [
    "# Exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOdqVEOp9Zbz"
   },
   "source": [
    "Créons un jeu de données $\\bf x$ composés de 100 séquences de 4 nombres entre 0 et 100. Les cibles $\\bf y$ correspondent à la somme de chacune de ces séquences. La fonction *data_set* située dans la section *Module utilitaire* s'occupe de cette tâche fastidieuse. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "14lZPQwTQek8"
   },
   "outputs": [],
   "source": [
    "# Création d'un jeu de donnés \n",
    "x,y = data_set(100, 4, [-100, 100])\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4YwIajCYJ5vx"
   },
   "outputs": [],
   "source": [
    "sequence = print_sequence(x[1,:] ,y[1])\n",
    "\n",
    "print(sequence)\n",
    "print('Dimensions des entrées : {} x {}'.format(*x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTKwBXDmqoqk"
   },
   "source": [
    "__Q1__ : Selon l'exemple ci-haut, quels sont les paramètres d'entrées? On considéra ici qu'un seul lot (*batch*). \n",
    "\n",
    "* batch_size = \n",
    "* seq_len = \n",
    "* input = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RÉPONSE: \n",
    "x_ = x[:,:,np.newaxis]\n",
    "y_ = y[:,np.newaxis].type(torch.FloatTensor)\n",
    "\n",
    "# ...\n",
    "\n",
    "print('Dimensions du tenseur x = {}'.format(x_.shape))\n",
    "print('______________________________________________')\n",
    "print('    ')\n",
    "\n",
    "print('Plus en détails: ')\n",
    "print('_________________')\n",
    "print('    ')\n",
    "\n",
    "dimensions = x_.shape\n",
    "print('batch_size = {}   seq_len = {}     input_size = {}'.format(*dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAJ9tNujqoql"
   },
   "source": [
    "__Q2__  : Donnez un exemple où les dimensions d'entrées sont supérieures à un, c.-à-d. *input* $> 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Y4AW07Jqoqq"
   },
   "source": [
    "__Q3__  : Construisez votre première couche LSTM. Nous vous référons au lien dessous la Fig. 3 pour plus de détails et exemples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0zyJ2FFBJ57d"
   },
   "outputs": [],
   "source": [
    "# Q3\n",
    "# les paramètres\n",
    "input_size = 1\n",
    "hidden_size = 6\n",
    "num_layers = 1\n",
    "\n",
    "# lstm\n",
    "#lstm = ...\n",
    "\n",
    "# forward \n",
    "output_0, (hn, cn) = lstm(x_)\n",
    "\n",
    "print('Dimensions - output_0: {}'.format(output_0.shape))\n",
    "print('Dimensions - h_n: {}'.format(hn.shape))\n",
    "print('Dimensions - c_n: {}'.format(cn.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MX0t3nT98kAn"
   },
   "source": [
    "### Réorganiser les dimensions de la sortie (\"<font color='red'>output_0</font>\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIVvInzN8zqm"
   },
   "source": [
    "Tel que mentionné ci-haut, seule la dernière composante du tenseur \"output_0\" sera considérée pour prédire la cible associée à une séquence (voir Fig.2). \n",
    "\n",
    "Exemple: considérons la $41^e$ séquence ainsi que la couche associée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "w50E_nbBJ5-e"
   },
   "outputs": [],
   "source": [
    "print(x_[41,:,:])\n",
    "print(y_[41])\n",
    "\n",
    "sequence = print_sequence(x[41,:],y[41])\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "F9ZExe-iJ6BE"
   },
   "outputs": [],
   "source": [
    "print(output_0[41,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vat-3CyN9j9y"
   },
   "source": [
    "Tel que mentionné ci-haut, seule la quatrième (dernière) composante du tenseur output_0 qui doit être considéré (voir Fig. 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Jt5sipkq97_b"
   },
   "outputs": [],
   "source": [
    "output_0[41,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDD1ICcJL_B7"
   },
   "source": [
    "Le tenseur $h_n$ regroupe la dernière sortie (*output*) de chaque séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7rh_45NWMbqT"
   },
   "outputs": [],
   "source": [
    "print(hn[:,41,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ewppj5yvsVJT"
   },
   "source": [
    "__Q4__  : Proposez une stratégie pour transformer le tenseur \"output_0\" de dimensions [100,4,6] à un tenseur de dimensions voulues, c.-à-d. [100,1]. Spécifiquement, quelle opération mathématique permettra d'obtenir les dimensions voulues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BaS5Ku1GJ6Dr"
   },
   "outputs": [],
   "source": [
    "## Question\n",
    "\n",
    "'''\n",
    "Première partie: générez un tenseur M de dimensions voulues et utilisez \n",
    "la fonction 'torch.matmul' pour  multiplier M et output_0. \n",
    "\n",
    "M = torch.Tensor( _?_, _?_ )\n",
    "output = torch.matmul(output_0[_?_, _?_, _?_], M)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Deuxième partie: gérérez une couche linéaire de dimensions voulues avec la \n",
    "fonction 'LL = nn.Linear(_?_, _?_)'. \n",
    "\n",
    "L'entrée de LL sera output_0, i.e. output = LL(output_0[_?_, _?_, _?_])\n",
    "'''\n",
    "\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnWwn0qbAjQG"
   },
   "source": [
    "### Construction du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af7055DXtEq9"
   },
   "source": [
    "Il est fortement suggéré d'utiliser des classes pour définir les modèles dans l'environnement PyTorch, et ce peu importe son architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjTN6N9pB96s"
   },
   "source": [
    "__Q5__  : Écrivez la fonction RnnLinear.forward(). Cette fonction aura comme entrée *self* et le tenseur $\\bf x$ de dimension N x T x 1. \n",
    "\n",
    "Indice : le output_0 de la couche LSTM ou RNN sera l'entrée de la couche linéaire. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YRLdrWvNJ6Kz"
   },
   "outputs": [],
   "source": [
    "class RnnLinear(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, lstm_or_rnn,cuda):\n",
    "        '''\n",
    "        Les paramètres importants du modèle sont définies dans cette fonction. \n",
    "        '''\n",
    "        num_directions = 1\n",
    "        super(RnnLinear, self).__init__()\n",
    "        \n",
    "        # parametres importants \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_or_rnn = lstm_or_rnn      \n",
    "        \n",
    "        # couches de notre réseau \n",
    "        if lstm_or_rnn == 'lstm':\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, \n",
    "                                num_layers,dropout = 0.5,\n",
    "                                batch_first = True)\n",
    "            if cuda : self.lstm.cuda() \n",
    "              \n",
    "        elif lstm_or_rnn == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size,\n",
    "                              num_layers,dropout = 0.5, \n",
    "                              batch_first = True\n",
    "                             )\n",
    "            if cuda: self.rnn.cuda()\n",
    "              \n",
    "        else: print('You made a mistake, pal!')\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, input_size,bias=True)\n",
    "        if cuda: \n",
    "            self.linear.cuda()\n",
    "              \n",
    "        # criteria for SGD\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def input_format(self,x,y):\n",
    "        '''\n",
    "        Cette fonction permet de préparer les dimensions des données d'entrées. \n",
    "        '''\n",
    "        x = x[:,:,None]\n",
    "        y = y[:,None].type(torch.FloatTensor)\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        return x,y\n",
    "\n",
    "    def grad_norm(self):\n",
    "        total_norm=0\n",
    "        for p in list(self.parameters()):\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item()\n",
    "\n",
    "        return total_norm\n",
    "  \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Cette fonction devrait inclure des conditions if/elif sur la variable \n",
    "        self.lstm_or_rnn. La sortie des couches LSTM et RNN seront les entrées \n",
    "        de la couche linéaire. \n",
    "        '''\n",
    "        \n",
    "        # cas où self.lstm_or_rnn = 'rnn'\n",
    "        if self.lstm_or_rnn == 'rnn':\n",
    "            # out1,(__) = ...\n",
    "            pass\n",
    "\n",
    "        # cas où self.lstm_or_rnn = 'lstm'      \n",
    "        if self.lstm_or_rnn == 'lstm':\n",
    "            # out1,(__) = ...\n",
    "            pass\n",
    "\n",
    "        # votre couche linéraire             \n",
    "        # out2 = ...\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFBy2S82DCS2"
   },
   "source": [
    "Testons notre modèle!\n",
    "\n",
    "Prenez le temps de bien comprendre les dimensions affichées ci-bas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ee-kXlLaDKKV"
   },
   "outputs": [],
   "source": [
    "x,y = data_set(100, 4, [0,100])\n",
    "\n",
    "\n",
    "input_size = 1\n",
    "num_layers = 2\n",
    "hidden_size = 40 \n",
    "\n",
    "model = RnnLinear(input_size, hidden_size, num_layers, 'lstm', cuda)\n",
    "\n",
    "xx,yy = model.input_format(x, y)\n",
    "print('Dimensions initiales des données : {}'.format(x.shape))\n",
    "print('Dimensions des données formatées : {}'.format(xx.shape))\n",
    "\n",
    "pred = model(xx)\n",
    "print('Dimensions des prédictions : {}'.format(pred.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMsFy8V1B0vk"
   },
   "source": [
    "### Questions\n",
    "\n",
    "__Q6__\n",
    "\n",
    "Entrainez le modèle sur 20 époques avec ces paramètres et hyperparamètres, c.-à-dire $N_{train}$ = 20000, T = 10, intervalle = [0,1000], $h_d$ = 20, num_layers = 2, lr0 = 0.05. \n",
    "\n",
    "__Q7__ : \n",
    "\n",
    "Testez le modèle sur l'ensemble de test. Commentez les résultats obtenus.  \n",
    "\n",
    "Note: L1 loss est définie comme la somme des différences absolue entre les cibles et les prédictions. Pour l'ensemble de test considéré, cette quantité est définie comme \\begin{align}\n",
    "L1 = \\sum_{i=1}^{5000} \\mid y^{(i)} - \\text{output}^{(i)}\\mid \\end{align}\n",
    "\n",
    "__Q8__ \n",
    "\n",
    "Finalement, *'standardisez'* les données de chaque séquence. Vous devrez utiliser la fonction *x, y = standardized(x, y)* définie dans le module utilitaire. \n",
    "\n",
    "__Q9__\n",
    "\n",
    "Augmenter le nombre d'epoch à 100. Observez l'apprentissage.\n",
    "\n",
    "__Q10__\n",
    "\n",
    "Il est possible d'activer ou de désactiver l'option dropout de notre modèle en appelant 'model.train()' et 'model.eval()' respectivement. Testez ces deux modes. \n",
    "\n",
    "Pourquoi est-il nécessaire de désactiver l'option 'dropout' lors des phases test et validation? \n",
    "\n",
    "__Q11__ \n",
    "\n",
    "Explorez différents hyperparamètres. \n",
    "\n",
    "* lr = 0.1, 1, 0.00001\n",
    "\n",
    "* hidden_size = 100,150, 200\n",
    "\n",
    "* dropout = 0.1, 0.3, 0.5\n",
    "\n",
    "Enjoy! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnV7bUgZsvz7"
   },
   "source": [
    "### Création du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "c8AsbJZmJ6OB"
   },
   "outputs": [],
   "source": [
    "x,y = data_set(20000, 10, [0,1000])\n",
    "\n",
    "#  standardized\n",
    "xtrain, ytrain = (x[:10000], y[:10000])\n",
    "xvalid, yvalid = (x[10000:15000], y[10000:15000])\n",
    "xtest, ytest = (x[15000:], y[15000:])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "all_data_train = data_utils.DataLoader(data_utils.TensorDataset(xtrain, ytrain),\n",
    "                                       batch_size, shuffle=True)\n",
    "all_data_valid = data_utils.DataLoader(data_utils.TensorDataset(xvalid, yvalid), \n",
    "                                       batch_size, shuffle=False)\n",
    "all_data_test = data_utils.DataLoader(data_utils.TensorDataset(xtest, ytest),\n",
    "                                       batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxVLLPwMFh0o"
   },
   "source": [
    "### Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "srcOUXeqQrZl"
   },
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "num_layers = 2\n",
    "hidden_size = 20\n",
    "number_epoch = 20\n",
    "lr0 = 0.05\n",
    "\n",
    "model = RnnLinear(input_size, hidden_size, num_layers,'lstm',cuda)\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr0)\n",
    "\n",
    "# tableau à remplir lors de la phase d'entrainement \n",
    "loss_train_data = np.array([]) \n",
    "loss_valid_data = np.array([]) \n",
    "\n",
    "total_norm = np.array([])\n",
    "\n",
    "t0 = time()  \n",
    "\n",
    "for e_ in range(number_epoch):\n",
    "  \n",
    "    # model.train(), poruquoi? Indice : dropout   \n",
    "    model.train()\n",
    "    for batch_idx, (xx, yy) in enumerate(all_data_train):                \n",
    "        # Formatez les données\n",
    "        xx,yy = model.input_format(xx,yy)\n",
    "\n",
    "        # Évaluez le modèle \n",
    "        pred_batch = model(xx)\n",
    "\n",
    "        # Initialisez les gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculez la loss  \n",
    "        loss_batch = model.criterion(pred_batch,yy)\n",
    "\n",
    "        # SGD backward \n",
    "        loss_batch.backward()\n",
    "\n",
    "        # SGD optimized\n",
    "        optimizer.step()\n",
    "\n",
    "    # stop. \n",
    "  \n",
    "    # Calculez la norm de tous les gradients \n",
    "    total_norm = np.append(total_norm,model.grad_norm())\n",
    "\n",
    "    # model.eval(), pourquoi? Indice : dropout \n",
    "    model.eval()  \n",
    "\n",
    "    loss_train_data = np.append(loss_train_data, batch_loss(all_data_train,model)) \n",
    "    loss_valid_data = np.append(loss_valid_data, batch_loss(all_data_valid,model))\n",
    "\n",
    "    if e_%10 == 0: \n",
    "        # '{} {}'.format('foo', 'bar')\n",
    "        print('N. of Epochs = {}, Train Loss = {}, Valid Loss = {}'.format(e_, loss_train_data[e_], loss_valid_data[e_]))\n",
    "\n",
    "\n",
    "    lr_ = adjust_lr(optimizer, lr0, e_, number_epoch)\n",
    "\n",
    "tf = time()\n",
    "print('Terminé, {} sec'.format(tf-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16ZBNnYKFJCS"
   },
   "source": [
    "### Prédiction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_r19roAVGeSk"
   },
   "outputs": [],
   "source": [
    "# prêt pour évaluer\n",
    "model.eval()\n",
    "\n",
    "# prêt pour entraîner\n",
    "# model.train()\n",
    "\n",
    "xtest_,ytest_ = model.input_format(xtest, ytest)\n",
    "pred_test_all = model(xtest_)\n",
    "\n",
    "L1_loss = torch.sum(torch.abs(pred_test_all - ytest_)).item()\n",
    "\n",
    "print('LSTM L1 loss = {}'.format(L1_loss))\n",
    "print('      ')\n",
    "print('Prediction')\n",
    "print('___________________')\n",
    "print(pred_test_all[0:10])\n",
    "print('      ')\n",
    "print('Target')\n",
    "print('___________________')\n",
    "print(ytest_[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfQB8XPdF8wO"
   },
   "source": [
    "### Courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKcZTByAxCjk"
   },
   "source": [
    "Nous traçons plus bas deux graphiques d'importance capitale : \n",
    "\n",
    "* Courbe d'apprentissage, i.e. loss vs. époque pour l'ensemble de validation et d'entrainement. \n",
    "* La norme de tous les gradients évaluée à chaque époque. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dygg3CwaKwxw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,6))\n",
    "ax1.plot(loss_train_data,'-k',label='Train')\n",
    "ax1.plot(loss_valid_data,'-r',label='Valid')\n",
    "\n",
    "ax2.plot(total_norm,'-b',label='Grad')\n",
    "\n",
    "\n",
    "ax1.set_xlabel('epoch',fontsize=14)\n",
    "ax2.set_xlabel('epoch',fontsize=14)\n",
    "ax1.set_ylabel('MSE loss',fontsize=14)\t\n",
    "ax2.set_ylabel('Grad Norm',fontsize=14)\t\n",
    "legend = ax1.legend(loc='upper right',fontsize=14)\n",
    "\n",
    "ax1.set_xlim((0))\n",
    "\n",
    "ax1.set_title('Learning curve', fontsize = 14)\n",
    "ax1.set_title('Learning curve', fontsize = 14)\n",
    "\n",
    "xmin, xmax = ax2.get_xlim()\n",
    "ymin, ymax = ax2.get_ylim()\n",
    "\n",
    "ax2.text(0.6*(xmax - xmin) + xmin, 0.8*(ymax-ymin) + ymin,\n",
    "         'num_layers : '+str(num_layers), fontsize=14)\n",
    "\n",
    "ax2.text(0.6*(xmax - xmin) + xmin, 0.72*(ymax-ymin) + ymin,\n",
    "         'hidden_size : '+str(hidden_size), fontsize=14)\n",
    "\n",
    "ax2.text(0.6*(xmax - xmin) + xmin, 0.64*(ymax-ymin) + ymin,\n",
    "         'number_epoch : '+str(number_epoch), fontsize=14)\n",
    "\n",
    "ax2.text(0.6*(xmax - xmin) + xmin, 0.56*(ymax-ymin) + ymin,\n",
    "         'lr$_0$ : '+str(lr0), fontsize=14)\n",
    "\n",
    "\n",
    "adjust_fontsize([ax1,ax2])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXLLEtnrGBdn"
   },
   "source": [
    "### Question\n",
    "\n",
    "__Q12__ : Dans le graphique ci-haut, comment expliquez-vous que la norme diminue et converge vers zéro avec le nombre d'époques? \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqPYtKa0E4UR"
   },
   "source": [
    "## Prédiction sur des séquences de longueurs différentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vKX2NMyF5oj"
   },
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EA9MAlcqaNae"
   },
   "source": [
    "__Q13__ : Prouvez qu'il est aussi possible d'utiliser le modèle entrainé pour prédire des séquences d'une longueur différente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xr5OPW0W7lnd"
   },
   "outputs": [],
   "source": [
    "T_= 12\n",
    "\n",
    "xx, yy = data_set(500,T_,[0,100])\n",
    "xx, yy = normalized(xx, yy)\n",
    "\n",
    "xx, yy = model.input_format(xx, yy)\n",
    "\n",
    "print('Séquence de longueur {}'.format(T_))\n",
    "print('--------------------------')\n",
    "print('Prédiction: {}'.format(model(xx).data[0][0]))\n",
    "\n",
    "print('Cible: {}'.format(yy.data[0][0]))\n",
    "print('LSTM L1 loss: {}'.format((torch.abs(model(xx).data[0]-yy.data[0])[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJ1XwmGVKGVa"
   },
   "source": [
    "# Comparaison entre RNN et LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMXnblC77hwv"
   },
   "source": [
    "\n",
    "__Q14__ : Entrainez le modèle ci-bas pour ces séries d'hyperparamètres et comparez les performances des modèles RNN et LSTM. \n",
    "\n",
    "* 'Stardardized' les données. \n",
    "* N = 30000\n",
    "* $h_d$ = 50\n",
    "* number_epoch = 100\n",
    "* batch_size = 100\n",
    "* lr0 = 0.005\n",
    "\n",
    "  * T = 20, 50, 100, 200\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BXM6MKtmKw7v"
   },
   "outputs": [],
   "source": [
    "cuda = True \n",
    "\n",
    "input_size = 1\n",
    "num_layers = 2\n",
    "hidden_size = 50\n",
    "number_epoch = 100\n",
    "batch_size = 100\n",
    "lr0 = 0.005\n",
    "\n",
    "loss_train_data_rnn = np.array([])\n",
    "loss_train_data_lstm = np.array([])\n",
    "\n",
    "loss_valid_data_rnn = np.array([])\n",
    "loss_valid_data_lstm = np.array([])\n",
    "\n",
    "total_norm_rnn = np.array([])\n",
    "total_norm_lstm = np.array([])\n",
    "\n",
    "# Génération des données\n",
    "T = 100\n",
    "\n",
    "x,y = data_set(30000, T, [-10000,10000])\n",
    "\n",
    "xtrain, ytrain = normalized(x[:20000], y[:20000])\n",
    "xvalid, yvalid = normalized(x[20000:25000], y[20000:25000])\n",
    "xtest, ytest = normalized(x[25000:], y[25000:])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "all_data_train = data_utils.DataLoader(data_utils.TensorDataset\n",
    "                          (xtrain, ytrain),batch_size, shuffle=True)\n",
    "  \n",
    "all_data_valid = data_utils.DataLoader(data_utils.TensorDataset\n",
    "                          (xvalid, yvalid),batch_size, shuffle=False)\n",
    "\n",
    "all_data_test = data_utils.DataLoader(data_utils.TensorDataset\n",
    "                          (xtest, ytest),batch_size, shuffle=False)\n",
    "\n",
    "# Initialisation des modèeles\n",
    "model_rnn = RnnLinear(input_size, hidden_size, num_layers, 'rnn', cuda)\n",
    "model_lstm = RnnLinear(input_size, hidden_size, num_layers, 'lstm', cuda)\n",
    "optimizer_rnn = optim.SGD(model_rnn.parameters(), lr=lr0)\n",
    "optimizer_lstm = optim.SGD(model_lstm.parameters(), lr=lr0)\n",
    "\n",
    "t0 = time()  \n",
    "for e_ in range(number_epoch):  \n",
    "    model_rnn.train()\n",
    "    \n",
    "    ##################\n",
    "    ###### RNN ######\n",
    "    ##################\n",
    "    \n",
    "    for batch_idx, (xx, yy) in enumerate(all_data_train):                \n",
    "        xx, yy = model_rnn.input_format(xx, yy)\n",
    "        pred_batch_rnn = model_rnn(xx)\n",
    "        optimizer_rnn.zero_grad()\n",
    "        loss_batch_rnn = model_rnn.criterion(pred_batch_rnn, yy)\n",
    "        loss_batch_rnn.backward()\n",
    "        optimizer_rnn.step()\n",
    "\n",
    "    model_rnn.eval()\n",
    "    lt = batch_loss(all_data_train, model_rnn)\n",
    "    lv = batch_loss(all_data_valid, model_rnn)\n",
    "    loss_train_data_rnn = np.append(loss_train_data_rnn, lt)\n",
    "    loss_valid_data_rnn = np.append(loss_valid_data_rnn, lv)\n",
    "\n",
    "    total_norm_rnn = np.append(total_norm_rnn,model_rnn.grad_norm())\n",
    "    \n",
    "    ##################\n",
    "    ###### LSTM ######\n",
    "    ##################\n",
    "\n",
    "    model_lstm.train()\n",
    "    for batch_idx, (xx, yy) in enumerate(all_data_train):\n",
    "        xx,yy = model_lstm.input_format(xx, yy)\n",
    "        pred_batch_lstm = model_lstm(xx)\n",
    "        optimizer_lstm.zero_grad()\n",
    "        loss_batch_lstm = model_lstm.criterion(pred_batch_lstm,yy)\n",
    "        loss_batch_lstm.backward()\n",
    "        optimizer_lstm.step()\n",
    "\n",
    "    \n",
    "    model_lstm.eval()\n",
    "    lt = batch_loss(all_data_train, model_lstm)\n",
    "    lv = batch_loss(all_data_valid, model_lstm)\n",
    "    loss_train_data_lstm = np.append(loss_train_data_lstm, lt)\n",
    "    loss_valid_data_lstm = np.append(loss_valid_data_lstm, lv)\n",
    "\n",
    "    total_norm_lstm = np.append(total_norm_lstm, model_lstm.grad_norm())  \n",
    "\n",
    "    ##################\n",
    "    ##################\n",
    "    \n",
    "    if e_%10 == 0: \n",
    "        print('N. of Epochs # {}, RNN, Train loss = {}  ----    LSTM, Train loss = {}'\n",
    "                .format(e_,loss_train_data_rnn[e_],loss_train_data_lstm[e_]))\n",
    "    \n",
    "    lr_ = adjust_lr(optimizer_rnn,lr0, e_, number_epoch)\n",
    "    lr_ = adjust_lr(optimizer_lstm,lr0, e_, number_epoch)\n",
    "\n",
    "    tf = time()\n",
    "print('Terminé, %.1f sec'%(tf - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cdmCKBO9jR_d"
   },
   "outputs": [],
   "source": [
    "fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(16,12))\n",
    "ax1.plot(loss_train_data_rnn, '-k', label ='train')\n",
    "ax1.plot(loss_valid_data_rnn, '-r', label = 'valid')\n",
    "\n",
    "ax2.plot(total_norm_rnn, '-k', label = 'norm rnn')\n",
    "\n",
    "ax3.plot(loss_train_data_lstm, '-k', label ='train')\n",
    "ax3.plot(loss_valid_data_lstm, '-r', label = 'valid')\n",
    "\n",
    "ax4.plot(total_norm_lstm, '-k', label = 'norm lstm')\n",
    "\n",
    "ax3.set_xlabel('epoch', fontsize=14)\n",
    "ax4.set_xlabel('epoch', fontsize=14)\n",
    "\n",
    "ax1.set_ylabel('MSE loss', fontsize=14)\t\n",
    "ax3.set_ylabel('MSE loss', fontsize=14)\t\n",
    "\n",
    "ax1.set_title('RNN', fontsize = 14)\n",
    "ax2.set_title('RNN', fontsize = 14)\n",
    "ax3.set_title('LSTM', fontsize = 14)\n",
    "ax4.set_title('LSTM', fontsize = 14)\n",
    "legend = ax1.legend(loc='upper right', fontsize=14)\n",
    "legend = ax3.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "\n",
    "ax1.set_xlim((0,number_epoch))\n",
    "ax1.set_ylim((0))\n",
    "ax2.set_ylim((0))\n",
    "ax3.set_xlim((0,number_epoch))\n",
    "ax3.set_ylim((0))\n",
    "ax4.set_ylim((0))\n",
    "\n",
    "xmin, xmax = ax1.get_xlim()\n",
    "ymin, ymax = ax1.get_ylim()\n",
    "\n",
    "ax1.text(0.1*(xmax - xmin) + xmin, 0.6*(ymax-ymin) + ymin,\n",
    "         'num_layers : %.f'%num_layers, fontsize=14)\n",
    "ax1.text(0.1*(xmax - xmin) + xmin, 0.52*(ymax-ymin) + ymin,\n",
    "         'hidden_size : %.f'%hidden_size, fontsize=14)\n",
    "ax1.text(0.1*(xmax - xmin) + xmin, 0.44*(ymax-ymin) + ymin,\n",
    "         'number_epoch : %.f'%number_epoch, fontsize=14)\n",
    "ax1.text(0.1*(xmax - xmin) + xmin, 0.36*(ymax-ymin) + ymin,\n",
    "         'lr$_0$ : %.1f'%lr0, fontsize=14)\n",
    "\n",
    "\n",
    "for ax in [ax1,ax2,ax3,ax4]:\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] \n",
    "                   + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(14)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "MILA_rnn_lstm_solution.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
