{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from deeplib.datasets import load_cifar10, load_mnist, train_valid_loaders\n",
    "from deeplib.net import CifarNet, CifarNetBatchNorm\n",
    "from deeplib.training import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Régularisation L1 et L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_dataset, _ = load_cifar10(True, '/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Implémentation manuelle\n",
    "La régularisation L2 est communément appelée **weight decay**. Dans PyTorch, les optimiseur de torch.optim ont un paramètre `weight_decay` pour facilement utiliser cette régularisation. Par contre, on peut facilement implémenter manuellement la régularisation L2 si on l'interprète comme une pénalité sur la norme des poids (voir le [chapitre 7.1](http://www.deeplearningbook.org/contents/regularization.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_init(parameters=[], reg_alpha=0):\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def loss_function(output, targets):\n",
    "        loss = cross_entropy(output,targets)\n",
    "        \n",
    "        for p in parameters:\n",
    "            # TODO\n",
    "            loss += reg_alpha * torch.norm(p)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    return loss_function\n",
    "    \n",
    "\n",
    "net = CifarNet()\n",
    "net.cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "loss = loss_init(net.parameters(), 1e-3)\n",
    "\n",
    "train(net, optimizer, train_dataset, n_epoch=5, batch_size=64, use_gpu=True, criterion=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "- Implémenter la régularisation L2\n",
    "- Implémenter la régularisation L1 manuellement (comme pour la L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### En utilisant weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 0.2\n",
    "n_epoch = 15\n",
    "decay = 1e-4\n",
    "\n",
    "net_without_l2 = CifarNet()\n",
    "net_without_l2.cuda()\n",
    "net_l2 = CifarNet()\n",
    "net_l2.cuda()\n",
    "\n",
    "optimizer_without_l2 = optim.SGD(net_without_l2.parameters(), lr=lr, weight_decay=0)\n",
    "optimizer_l2 = optim.SGD(net_l2.parameters(), lr=lr, weight_decay=decay)\n",
    "\n",
    "history_without_l2 = train(net_without_l2, optimizer_without_l2, train_dataset, n_epoch, batch_size, use_gpu=True)\n",
    "history_l2 = train(net_l2, optimizer_l2, train_dataset, n_epoch, batch_size, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_without_l2.display_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_l2.display_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "- Quel est l'effet de la régularisation L2 sur l'entraînement du réseau, plus spécifiquement l'accuracy?\n",
    "\n",
    "#### Exercice\n",
    "- Utilisez un weight_decay trop grand. Qu'arrive-t-il? Pourquoi?\n",
    "- Quel weight decay fonctionne le mieux pour vous? Un sondage sera fait lors du retour en classe à la fin du laboratoire.\n",
    "- Dans la cellule suivante, analysez la distribution des poids appris par le réseau (variance, moyenne, min, max). Faites un histogramme des poids. Que remarquez-vous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def net_to_weight_array(net):\n",
    "    weights = []\n",
    "    for p in net.parameters():\n",
    "        p_numpy = p.data.cpu().numpy()\n",
    "        weights.append(p_numpy.reshape((-1))) # Reshape to 1D array\n",
    "    return np.concatenate(weights)\n",
    "\n",
    "        \n",
    "# TODO\n",
    "weights_without_l2 = net_to_weight_array(net_without_l2)\n",
    "weights_l2 = net_to_weight_array(net_l2)\n",
    "\n",
    "print(np.average(weights_without_l2))\n",
    "print(np.average(weights_l2))\n",
    "print(np.var(weights_without_l2))\n",
    "print(np.var(weights_l2))\n",
    "print(np.max(np.abs(weights_without_l2)))\n",
    "print(np.max(np.abs(weights_l2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Early stopping\n",
    "\n",
    "Commencez par entraîner un réseau un grand nombre d'epoch. L'historique d'entraînement nous servira de base pour les questions qui suivent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "net = CifarNetBatchNorm()\n",
    "net.cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.2, weight_decay=1e-4, nesterov=True, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,15,40], gamma=0.5)\n",
    "\n",
    "history = train(net, optimizer, train_dataset, n_epoch=60, batch_size=64, use_gpu=True, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- En regardant les graphiques ci-dessus, quel est le meilleur moment pour arrêter l'entraînement? Pourquoi?\n",
    "- Identifiez des problèmes pratiques potentiels lors de l'utilisation de l'early stopping.\n",
    "\n",
    "#### Exercice\n",
    "L'algorithme 7.1 du livre (voir http://www.deeplearningbook.org/contents/regularization.html) décrit le paramètre de patience `p`. Analysez l'effet du choix de `p` sur les données de l'entraînement précédent. Regardez, pour `p = 1,5,10,15`, quel modèle avec quelle précision en validation est choisi. Utilisez les `val_accuracy` de l'entraînement que vous venez d'exécuter pour vos tests (à la place d'entraîner le réseau)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 1\n",
    "val_accuracy = history.history['val_acc']\n",
    "\n",
    "print(val_accuracy[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dropout\n",
    "Cette section a pour but d'analyser l'effet du dropout dans un réseau fully connected. Nous ferons cette analyse en reprenant l'exercice du laboratoire 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset, _ = load_mnist(True, '/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "Dans le réseau suivant, implémentez la fonction `forward()` en ajoutant du dropout si `self.use_dropout == True`. N'ajoutez **pas de softmax** car la fonction `deeplib.training.train()` utilise par défaut `CrossEntropyLoss`, ce qui le fait pour vous. Utilisez une probabilité de drop de `0.4`. Ne faites pas de dropout sur la dernière couche.\n",
    "\n",
    "> **ATTENTION!** Vous devez bien fixer l'argument `training` de dropout. Vous pouvez savoir si modèle est en entraînement ou en évaluation avec self.training.\n",
    "\n",
    "### Question\n",
    "- Quelle est l'importance de l'argument `training` de la fonction de dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MnistModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layers, hidden_size=100, use_dropout=True):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.use_dropout = use_dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.first_layer = nn.Linear(28*28,hidden_size)\n",
    "        self.first_layer.weight.data.normal_(0.0, math.sqrt(2 / 28*28))\n",
    "        self.first_layer.bias.data.fill_(0.0001)\n",
    "        \n",
    "        self.layers = []\n",
    "        for i in range(n_layers - 1):\n",
    "            layer = nn.Linear(hidden_size,hidden_size)\n",
    "            layer.weight.data.normal_(0.0, math.sqrt(2 / hidden_size))\n",
    "            layer.bias.data.fill_(0.0001)\n",
    "            self.layers.append(layer)\n",
    "            self.add_module('layer-%d' % i, layer)\n",
    "            \n",
    "        self.output_layer = nn.Linear(hidden_size,10)\n",
    "        self.output_layer.weight.data.normal_(0.0, math.sqrt(2 / hidden_size))\n",
    "        self.output_layer.bias.data.fill_(0.0001)\n",
    "\n",
    "        self.nonzero_grad_stats = None\n",
    "              \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        out = F.relu(self.first_layer.forward(x))\n",
    "        for i, l in enumerate(self.layers):\n",
    "            out = F.relu(layer(out))\n",
    "            if self.use_dropout and i < len(self.layers) - 1:\n",
    "                out = F.dropout(out, 0.4, training=self.training)\n",
    "        return self.output_layer.forward(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînez un réseau avec dropout et un réseau sans dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = MnistModel(2, use_dropout=False)\n",
    "net_dropout = MnistModel(2, use_dropout=True)\n",
    "net.cuda()\n",
    "net_dropout.cuda()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, nesterov=True, momentum=0.9)\n",
    "optimizer_dropout = optim.SGD(net_dropout.parameters(), lr=0.005, nesterov=True, momentum=0.9)\n",
    "\n",
    "history = train(net, optimizer, dataset, 20, batch_size=64)\n",
    "history_dropout = train(net_dropout, optimizer_dropout, dataset, 20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_dropout.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "- Essayez plusieurs valeurs de dropout et observez les effets.\n",
    "- Essayer d'avoir des valeurs différentes de probabilité de dropout pour chaque couche. Est-ce que cela améliore les résultats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glo4030-7030",
   "language": "python",
   "name": "glo4030-7030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
